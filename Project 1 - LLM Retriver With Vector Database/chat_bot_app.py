"""
RAG LLM app with history-aware retriever
Based on vector searching of the Bank Of Israel's "Proper conduct of banking business directive"
"""

import os
from typing import List

import streamlit as st
from streamlit_float import float_init, float_css_helper, float_parent
from streamlit_feedback import streamlit_feedback
from langchain.schema import HumanMessage, AIMessage
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.callbacks import collect_runs
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.documents import Document
from langchain_core.runnables import chain
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langsmith import Client

from dotenv import load_dotenv

load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
pinecone_api_key = os.getenv("PINECONE_API_KEY")
langchain_api_key = os.getenv("LANGCHAIN_API_KEY")
langsmith_client = Client(api_key=langchain_api_key)

MODEL_NAME = "gpt-4o-mini" # $0.150 / 1M input tokens
# MODEL_NAME = "gpt-4o" # $2.50 / 1M input tokens
# MODEL_NAME = "gpt-4-turbo" # $10.00 / 1M tokens
# MODEL_NAME = "gpt-3.5-turbo" # $3.000 / 1M input tokens

EMBEDDINGS_MODEL_NAME="text-embedding-3-small" # US$0.02 / 1M tokens
# EMBEDDINGS_MODEL_NAME="text-embedding-ada-002" # US$0.10 / 1M tokens

INDEX_NAME = 'test-index-1536'
MAX_TOKENS = 300

st.set_page_config(page_title="bot", page_icon="ğŸ¤–")
float_init(theme=True, include_unstable_primary=False)
st.title("AI bot - ××¡××›×™ × ×•×”×œ ×‘× ×§××™ ×ª×§×™×Ÿ")

@st.cache_resource
def local_css(file_name: str) -> None:
Â  Â  """Loads and applies custom CSS styles from a specified file."""
Â  Â  try:
Â  Â  Â  Â  with open(file_name, encoding='utf-8') as f:
Â  Â  Â  Â  Â  Â  st.markdown(f"<style>{f.read()}</style>", unsafe_allow_html=True)
Â  Â  except FileExistsError:
Â  Â  Â  Â  st.error(f"CSS file {file_name} not found")
local_css("style.css") # Load custom CSS

session_defaults = {
Â  Â  "number_of_docs":1,
Â  Â  "similarity_score_threshold":0.65,
Â  Â  "chat_history":[],
Â  Â  "contents":[],
Â  Â  "retrived_content":[],
Â  Â  "run_id":None,
Â  Â  "feedback_result" :None

}

for key, value in session_defaults.items():
Â  Â  if key not in st.session_state:
Â  Â  Â  Â  st.session_state[key] = value

@st.cache_resource(show_spinner=False)
def initialize_llm() -> ChatOpenAI:
Â  Â  """Initializes and returns a language model instance for use in the application."""
Â  Â  return ChatOpenAI(
Â  Â  Â  Â  Â  Â  Â  Â  api_key=openai_api_key,
Â  Â  Â  Â  Â  Â  Â  Â  model=MODEL_NAME,
Â  Â  Â  Â  Â  Â  Â  Â  max_tokens=MAX_TOKENS,
Â  Â  Â  Â  Â  Â  Â  Â  temperature=0.1,
Â  Â  Â  Â  Â  Â  Â  Â  streaming=True, Â # Enable streaming
Â  Â  Â  Â  Â  Â  Â  Â  )

@st.cache_resource(show_spinner=False)
def initialize_retriever(number_of_docs: int, Â threshold:float):
Â  Â  """
Â  Â  Initializes and returns a document retriever configured for similarity searches.
Â  Â  This function creates an OpenAI embeddings model and a Pinecone vector store.
Â  Â  Find similar documents based on the specified number of documents to retrieve.
Â  Â  """
Â  Â  embeddings_model = OpenAIEmbeddings(model=EMBEDDINGS_MODEL_NAME, openai_api_key=openai_api_key)
Â  Â  vectorstore = PineconeVectorStore.from_existing_index(embedding=embeddings_model, index_name=INDEX_NAME)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  @chain
Â  Â  def retriever_with_similarity_score(query: str, n_docs=number_of_docs, threshold_float=threshold) -> List[Document]:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  docs, scores = zip(*vectorstore.similarity_search_with_relevance_scores(query, k=n_docs,score_threshold=threshold_float))
Â  Â  Â  Â  Â  Â  for doc, score in zip(docs, scores):
Â  Â  Â  Â  Â  Â  Â  Â  doc.metadata["score"] = "{:.1%}".format(score)
Â  Â  Â  Â  except ValueError:
Â  Â  Â  Â  Â  Â  print("No ducoument retriverd due to threshold score limit")
Â  Â  Â  Â  Â  Â  return []
Â  Â  Â  Â  return docs
Â  Â  return retriever_with_similarity_score
Â  Â  # return vectorstore.as_retriever(search_type="similarity", search_kwargs={'k': number_of_docs})

def initialize_combined_chain(llm, retriever):
Â  Â  """
Â  Â  Initializes and returns a combined retrieval and question-answering chain.
Â  Â  It sets up the necessary prompts for processing user queries and retrieving relevant context.
Â  Â  """
Â  Â  history_template = """Given a chat history and the latest user question which might reference context in the chat history, 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  formulate a standalone question which can be understood without the chat history. Do NOT answer the question, 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  just reformulate it if needed and otherwise return it as is. 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Chat History: {chat_history}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  """
Â  Â  history_prompt = ChatPromptTemplate.from_messages(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  [
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("system", history_template),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  MessagesPlaceholder("chat_history"),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ("human", "{input}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ]
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  
Â  Â  qa_template = """
Â  Â  Â  Â  Â  Â  Â  Â  You are a helpful Legal advisory lawyer in the field of finance.
Â  Â  Â  Â  Â  Â  Â  Â  Your job is to return complete and accurate answers to the manager of a consumer credit company.
Â  Â  Â  Â  Â  Â  Â  Â  Answer the question based on the context provided below. 
Â  Â  Â  Â  Â  Â  Â  Â  If you're unable to answer the question, simply reply with "Sorry, I don't know the answer". 
Â  Â  Â  Â  Â  Â  Â  Â  When a user requests code output, respond with: 'Sorry, we don't allow code output.'
Â  Â  Â  Â  Â  Â  Â  Â  Context: {context}
Â  Â  Â  Â  Â  Â  Â  Â  Question: {input} Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  """
Â  Â  qa_prompt = ChatPromptTemplate.from_messages([("system", qa_template), MessagesPlaceholder("chat_history"), ("human", "{input}")])
Â  Â  qa_chain = create_stuff_documents_chain(llm, qa_prompt)
Â  Â  history_chain = create_history_aware_retriever(llm, retriever, history_prompt)
Â  Â  return create_retrieval_chain(history_chain, qa_chain)



def get_session_history(session_id: str) -> BaseChatMessageHistory:
Â  Â  if "chat_message_history" not in st.session_state:
Â  Â  Â  Â  st.session_state.chat_message_history = ChatMessageHistory()
Â  Â  return st.session_state.chat_message_history

def display_ai_response_context(context):
Â  Â  expander_style = """<style>@import "styles.css";</style>"""
Â  Â  for i, ai_response_content in enumerate(context):
Â  Â  Â  Â  expander_title = f"**{i+1}. {ai_response_content.metadata['title']} ({ai_response_content.metadata['date']})** Â - Â  **Similarity score: {ai_response_content.metadata['score']}**"
Â  Â  Â  Â  st.markdown(expander_style, unsafe_allow_html=True)
Â  Â  Â  Â  with st.expander(expander_title, expanded=False, icon=':material/picture_as_pdf:'):
Â  Â  Â  Â  Â  Â  st.write(ai_response_content.page_content)
Â  Â  Â  Â  Â  Â  st.write(ai_response_content.metadata['pdf_url'])

def display_chat_history():
Â  Â  for message in st.session_state.chat_history:
Â  Â  Â  Â  if isinstance(message, HumanMessage):
Â  Â  Â  Â  Â  Â  with st.chat_message("human"):
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(message.content)
Â  Â  Â  Â  elif isinstance(message, AIMessage):
Â  Â  Â  Â  Â  Â  with st.chat_message("ai"):
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(message.content)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  display_ai_response_context(message)
Â  Â  Â 

def chat_content_append():
Â  Â  st.session_state['contents'].append(st.session_state.content) 

def generate_response(chain, query):
Â  Â  for chunk in chain.stream({"input": query, "chat_history": st.session_state.chat_history}, config={"configurable": {"session_id": INDEX_NAME}}):
Â  Â  Â  Â  if (input_data := chunk.get("input")) is not None:
Â  Â  Â  Â  Â  Â  pass
Â  Â  Â  Â  if (context := chunk.get("context")) is not None:
Â  Â  Â  Â  Â  Â  st.session_state.retrived_content = context
Â  Â  Â  Â  if (answer_chunk := chunk.get("answer")) is not None:
Â  Â  Â  Â  Â  Â  yield answer_chunk Â # Yield each chunk of the answer


def processes_feedback(run_id=0):
Â  Â  score_mappings = {"ğŸ‘": 1, "ğŸ‘": 0,}
Â  Â  if st.session_state.feedback_result is not None:
Â  Â  Â  Â  thumb_score = st.session_state.feedback_result["score"]
Â  Â  Â  Â  score = score_mappings[thumb_score]
Â  Â  Â  Â  feedback_response = langsmith_client.create_feedback(
Â  Â  Â  Â  Â  Â  Â  Â  run_id,
Â  Â  Â  Â  Â  Â  Â  Â  key="feedback-key",
Â  Â  Â  Â  Â  Â  Â  Â  score=score
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  print(f"feeback create for llm response {run_id} with score {score}/1")


def feedback(id):
Â  Â  with st.form(key='fb_form',border=False):
Â  Â  Â  Â  streamlit_feedback(
Â  Â  Â  Â  Â  Â  feedback_type="thumbs",
Â  Â  Â  Â  Â  Â  key="feedback_result",
Â  Â  Â  Â  Â  Â  # key=f"feedback_{run_id}",
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  st.form_submit_button('Send', on_click=processes_feedback, args=(id,))


def main()->None:
Â  Â  llm = initialize_llm()
Â  Â  retriever = initialize_retriever(st.session_state.number_of_docs, st.session_state.similarity_score_threshold)
Â  Â  combined_chain = initialize_combined_chain(llm, retriever)
Â  Â  final_chain = RunnableWithMessageHistory(
Â  Â  Â  Â  combined_chain,
Â  Â  Â  Â  get_session_history,
Â  Â  Â  Â  input_messages_key="input", 
Â  Â  Â  Â  output_messages_key="answer",
Â  Â  Â  Â  history_messages_key="chat_history",
Â  Â  Â  Â  )
Â  Â  

Â  Â  col1, col2 = st.columns([1, 4])
Â  Â  with col1:
Â  Â  Â  Â  with st.container(border=False):
Â  Â  Â  Â  Â  Â  st.number_input(
Â  Â  Â  Â  Â  Â  Â  Â  label="Number of Documents",
Â  Â  Â  Â  Â  Â  Â  Â  min_value=1,
Â  Â  Â  Â  Â  Â  Â  Â  max_value=7,
Â  Â  Â  Â  Â  Â  Â  Â  key="number_of_docs",
Â  Â  Â  Â  Â  Â  Â  Â  step=1,
Â  Â  Â  Â  Â  Â  Â  Â  format="%d"
Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  if st.button("Restart Chat"):
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.chat_history = []
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.expander_states = {}
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()

Â 
Â  Â  Â  Â  Â  Â  st.number_input(
Â  Â  Â  Â  Â  Â  Â  Â  label="Similarity score threshold",
Â  Â  Â  Â  Â  Â  Â  Â  min_value=0.01,
Â  Â  Â  Â  Â  Â  Â  Â  max_value=1.00,
Â  Â  Â  Â  Â  Â  Â  Â  key="similarity_score_threshold",
Â  Â  Â  Â  Â  Â  Â  Â  step=0.05,
Â  Â  Â  Â  Â  Â  )

Â  Â  with col2:
Â  Â  Â  Â  with st.container(border=False):
Â  Â  Â  Â  Â  Â  with st.container():
Â  Â  Â  Â  Â  Â  Â  Â  user_query = st.chat_input("××” ×©××œ×ª×š? ", key='content', on_submit=chat_content_append, max_chars=4000)
Â  Â  Â  Â  Â  Â  Â  Â  button_css = float_css_helper(width="2.2rem", bottom="0rem", transition=0)
Â  Â  Â  Â  Â  Â  Â  Â  float_parent(css=button_css)

Â  Â  Â  Â  Â  Â  display_chat_history()

Â  Â  Â  Â  Â  Â  if user_query is not None and user_query !="":
Â  Â  Â  Â  Â  Â  Â  Â  st.empty()
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.chat_history.append(HumanMessage(user_query))

Â  Â  Â  Â  Â  Â  Â  Â  with st.chat_message("human"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(user_query)
Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  with st.chat_message("ai"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with collect_runs() as cb:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ai_response = st.write_stream(generate_response(final_chain, user_query))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  run_id = cb.traced_runs[0].id

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.chat_history.append(AIMessage(ai_response))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.chat_history.append(st.session_state.retrived_content)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  display_ai_response_context(st.session_state.retrived_content)
Â  Â  Â  Â  Â  Â  

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if run_id is not None:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  feedback(run_id)


Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  





if __name__ == "__main__":
Â  Â  main()

